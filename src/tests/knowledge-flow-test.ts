import { hybridExtract, validateExtraction } from '../lib/ums/hybrid';
import { KNOWLEDGE_SCENARIOS, KnowledgeScenario } from './knowledge-flow-scenarios';

export interface TestResult {
  scenario: string;
  success: boolean;
  metrics: {
    totalEntities: number;
    expectedEntities: number;
    matchedEntities: number;
    precision: number;
    recall: number;
    f1Score: number;
    ruleMs: number;
    llmMs: number;
    totalMs: number;
    fallbackRate: number;
  };
  validation: {
    valid: boolean;
    issues: string[];
  };
  reasoning?: any[];
  errors: string[];
}

export interface BenchmarkSummary {
  totalScenarios: number;
  successfulScenarios: number;
  averagePrecision: number;
  averageRecall: number;
  averageF1: number;
  averageRuleMs: number;
  averageLlmMs: number;
  averageTotalMs: number;
  averageFallbackRate: number;
  totalErrors: number;
}

/**
 * Run a single knowledge flow test scenario
 */
export async function runScenarioTest(
  scenario: KnowledgeScenario,
  options: {
    explainReasoning?: boolean;
    persist?: boolean;
    verbose?: boolean;
  } = {}
): Promise<TestResult> {
  const { explainReasoning = true, persist = false, verbose = false } = options;
  const errors: string[] = [];
  
  try {
    if (verbose) {
      console.log(`\nðŸ§ª Testing scenario: ${scenario.name}`);
      console.log(`ðŸ“ Input length: ${scenario.input.length} chars`);
    }
    
    // Run hybrid extraction
    const startTime = Date.now();
    const result = await hybridExtract(scenario.input, {
      project: 'test',
      persist,
      explainReasoning,
      confidenceThreshold: 0.7,
    });
    
    // Validate extraction
    const validation = validateExtraction(result);
    
    // Calculate metrics
    const extractedEntityNames = new Set(
      result.entities.map(e => e.name.toLowerCase())
    );
    const expectedEntityNames = new Set(
      scenario.expectedEntities.map(e => e.name.toLowerCase())
    );
    
    let matchedCount = 0;
    expectedEntityNames.forEach(expected => {
      if (extractedEntityNames.has(expected)) {
        matchedCount++;
      } else if (verbose) {
        errors.push(`Missing expected entity: ${expected}`);
      }
    });
    
    const precision = result.entities.length > 0 
      ? matchedCount / result.entities.length 
      : 0;
    const recall = scenario.expectedEntities.length > 0
      ? matchedCount / scenario.expectedEntities.length
      : 0;
    const f1Score = precision + recall > 0
      ? 2 * (precision * recall) / (precision + recall)
      : 0;
    
    if (verbose) {
      console.log(`âœ… Extracted: ${result.entities.length} entities`);
      console.log(`ðŸ“Š Precision: ${(precision * 100).toFixed(1)}%`);
      console.log(`ðŸ“Š Recall: ${(recall * 100).toFixed(1)}%`);
      console.log(`ðŸ“Š F1 Score: ${(f1Score * 100).toFixed(1)}%`);
      console.log(`â±ï¸ Total time: ${result.metrics?.totalMs}ms`);
      
      if (result.metrics?.llmMs > 0) {
        console.log(`  - Rule extraction: ${result.metrics.ruleMs}ms`);
        console.log(`  - LLM extraction: ${result.metrics.llmMs}ms`);
        console.log(`  - Fallback rate: ${(result.metrics.fallbackRate * 100).toFixed(1)}%`);
      }
    }
    
    return {
      scenario: scenario.name,
      success: f1Score >= 0.7 && validation.valid,
      metrics: {
        totalEntities: result.entities.length,
        expectedEntities: scenario.expectedEntities.length,
        matchedEntities: matchedCount,
        precision,
        recall,
        f1Score,
        ruleMs: result.metrics?.ruleMs || 0,
        llmMs: result.metrics?.llmMs || 0,
        totalMs: result.metrics?.totalMs || (Date.now() - startTime),
        fallbackRate: result.metrics?.fallbackRate || 0,
      },
      validation,
      reasoning: explainReasoning ? result.reasoning : undefined,
      errors,
    };
  } catch (error) {
    errors.push(`Test failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    
    return {
      scenario: scenario.name,
      success: false,
      metrics: {
        totalEntities: 0,
        expectedEntities: scenario.expectedEntities.length,
        matchedEntities: 0,
        precision: 0,
        recall: 0,
        f1Score: 0,
        ruleMs: 0,
        llmMs: 0,
        totalMs: 0,
        fallbackRate: 0,
      },
      validation: {
        valid: false,
        issues: errors,
      },
      errors,
    };
  }
}

/**
 * Run all knowledge flow test scenarios
 */
export async function runAllScenarios(
  options: {
    explainReasoning?: boolean;
    persist?: boolean;
    verbose?: boolean;
  } = {}
): Promise<{
  results: TestResult[];
  summary: BenchmarkSummary;
}> {
  const results: TestResult[] = [];
  
  console.log('ðŸš€ Starting Knowledge Flow Validation Tests\n');
  console.log('=' .repeat(60));
  
  for (const [key, scenario] of Object.entries(KNOWLEDGE_SCENARIOS)) {
    const result = await runScenarioTest(scenario, options);
    results.push(result);
    
    // Add delay to avoid rate limiting
    await new Promise(resolve => setTimeout(resolve, 1000));
  }
  
  // Calculate summary statistics
  const summary: BenchmarkSummary = {
    totalScenarios: results.length,
    successfulScenarios: results.filter(r => r.success).length,
    averagePrecision: 
      results.reduce((sum, r) => sum + r.metrics.precision, 0) / results.length,
    averageRecall:
      results.reduce((sum, r) => sum + r.metrics.recall, 0) / results.length,
    averageF1:
      results.reduce((sum, r) => sum + r.metrics.f1Score, 0) / results.length,
    averageRuleMs:
      results.reduce((sum, r) => sum + r.metrics.ruleMs, 0) / results.length,
    averageLlmMs:
      results.reduce((sum, r) => sum + r.metrics.llmMs, 0) / results.length,
    averageTotalMs:
      results.reduce((sum, r) => sum + r.metrics.totalMs, 0) / results.length,
    averageFallbackRate:
      results.reduce((sum, r) => sum + r.metrics.fallbackRate, 0) / results.length,
    totalErrors:
      results.reduce((sum, r) => sum + r.errors.length, 0),
  };
  
  console.log('\n' + '=' .repeat(60));
  console.log('ðŸ“Š BENCHMARK SUMMARY');
  console.log('=' .repeat(60));
  console.log(`âœ… Success Rate: ${summary.successfulScenarios}/${summary.totalScenarios} scenarios`);
  console.log(`ðŸ“ˆ Average Precision: ${(summary.averagePrecision * 100).toFixed(1)}%`);
  console.log(`ðŸ“ˆ Average Recall: ${(summary.averageRecall * 100).toFixed(1)}%`);
  console.log(`ðŸ“ˆ Average F1 Score: ${(summary.averageF1 * 100).toFixed(1)}%`);
  console.log(`â±ï¸ Average Total Time: ${summary.averageTotalMs.toFixed(0)}ms`);
  console.log(`  - Rule extraction: ${summary.averageRuleMs.toFixed(0)}ms`);
  console.log(`  - LLM extraction: ${summary.averageLlmMs.toFixed(0)}ms`);
  console.log(`ðŸ”„ Average Fallback Rate: ${(summary.averageFallbackRate * 100).toFixed(1)}%`);
  console.log(`âŒ Total Errors: ${summary.totalErrors}`);
  
  return { results, summary };
}

/**
 * Generate a detailed report
 */
export function generateReport(
  results: TestResult[],
  summary: BenchmarkSummary
): string {
  const timestamp = new Date().toISOString();
  
  let report = `# Milestone D5 - Operational AI Validation Report\n\n`;
  report += `Generated: ${timestamp}\n\n`;
  
  report += `## Executive Summary\n\n`;
  report += `The hybrid extraction pipeline was tested against ${summary.totalScenarios} real-world scenarios `;
  report += `representing typical knowledge capture workflows at CBRT.\n\n`;
  
  report += `### Key Metrics\n\n`;
  report += `| Metric | Value |\n`;
  report += `|--------|-------|\n`;
  report += `| Success Rate | ${((summary.successfulScenarios / summary.totalScenarios) * 100).toFixed(1)}% |\n`;
  report += `| Average Precision | ${(summary.averagePrecision * 100).toFixed(1)}% |\n`;
  report += `| Average Recall | ${(summary.averageRecall * 100).toFixed(1)}% |\n`;
  report += `| Average F1 Score | ${(summary.averageF1 * 100).toFixed(1)}% |\n`;
  report += `| Average Processing Time | ${summary.averageTotalMs.toFixed(0)}ms |\n`;
  report += `| LLM Fallback Rate | ${(summary.averageFallbackRate * 100).toFixed(1)}% |\n\n`;
  
  report += `## Scenario Results\n\n`;
  
  results.forEach(result => {
    report += `### ${result.scenario}\n\n`;
    report += `- **Status**: ${result.success ? 'âœ… PASSED' : 'âŒ FAILED'}\n`;
    report += `- **Entities Found**: ${result.metrics.totalEntities} (Expected: ${result.metrics.expectedEntities})\n`;
    report += `- **Precision**: ${(result.metrics.precision * 100).toFixed(1)}%\n`;
    report += `- **Recall**: ${(result.metrics.recall * 100).toFixed(1)}%\n`;
    report += `- **F1 Score**: ${(result.metrics.f1Score * 100).toFixed(1)}%\n`;
    report += `- **Processing Time**: ${result.metrics.totalMs}ms`;
    
    if (result.metrics.llmMs > 0) {
      report += ` (Rule: ${result.metrics.ruleMs}ms, LLM: ${result.metrics.llmMs}ms)`;
    }
    report += `\n`;
    
    if (!result.validation.valid) {
      report += `- **Validation Issues**:\n`;
      result.validation.issues.forEach(issue => {
        report += `  - ${issue}\n`;
      });
    }
    
    if (result.errors.length > 0) {
      report += `- **Errors**:\n`;
      result.errors.forEach(error => {
        report += `  - ${error}\n`;
      });
    }
    
    report += `\n`;
  });
  
  report += `## Performance Analysis\n\n`;
  
  const ruleOnlyTime = summary.averageRuleMs;
  const hybridTime = summary.averageTotalMs;
  const speedup = ruleOnlyTime > 0 ? hybridTime / ruleOnlyTime : 1;
  
  report += `### Processing Speed\n\n`;
  report += `- Rule-only extraction: ${ruleOnlyTime.toFixed(0)}ms average\n`;
  report += `- Hybrid extraction: ${hybridTime.toFixed(0)}ms average\n`;
  report += `- Overhead factor: ${speedup.toFixed(2)}x\n\n`;
  
  report += `### Accuracy vs Speed Trade-off\n\n`;
  report += `The hybrid approach shows a ${speedup.toFixed(2)}x processing overhead compared to rule-only extraction, `;
  report += `but achieves ${(summary.averageF1 * 100).toFixed(1)}% F1 score compared to expected entities.\n\n`;
  
  report += `## Bottlenecks and Weak Spots\n\n`;
  
  const failedScenarios = results.filter(r => !r.success);
  if (failedScenarios.length > 0) {
    report += `### Failed Scenarios\n\n`;
    failedScenarios.forEach(scenario => {
      report += `- **${scenario.scenario}**: F1 Score ${(scenario.metrics.f1Score * 100).toFixed(1)}%\n`;
    });
    report += `\n`;
  }
  
  const slowScenarios = results.filter(r => r.metrics.totalMs > 2000);
  if (slowScenarios.length > 0) {
    report += `### Slow Processing (>2s)\n\n`;
    slowScenarios.forEach(scenario => {
      report += `- **${scenario.scenario}**: ${scenario.metrics.totalMs}ms\n`;
    });
    report += `\n`;
  }
  
  report += `## Recommendations\n\n`;
  report += `1. **Optimize LLM calls**: Current fallback rate of ${(summary.averageFallbackRate * 100).toFixed(1)}% could be reduced with better rule patterns\n`;
  report += `2. **Improve entity normalization**: Some entities are missed due to variation in naming\n`;
  report += `3. **Add caching**: Repeated entities across documents could benefit from caching\n`;
  report += `4. **Enhance relationship extraction**: Current focus is on entities; relationships need more work\n\n`;
  
  report += `## Conclusion\n\n`;
  report += `The hybrid extraction pipeline is operational and shows promising results with an average F1 score of `;
  report += `${(summary.averageF1 * 100).toFixed(1)}%. The system successfully handles real-world scenarios including `;
  report += `coil scans, bills of lading, and regulatory documents. With the identified optimizations, the system `;
  report += `is ready for production deployment with appropriate monitoring.\n`;
  
  return report;
}